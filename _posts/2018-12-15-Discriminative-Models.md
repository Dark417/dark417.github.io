---
published: true
layout: single
mathjax: true
toc: true
toc_sticky: true
category: Machine Learning
excerpt: "This post is a translation for one of Wei's posts in his machine learning notes."
title: Discriminative Learning Algorithm / 分类学习算法
---
Please note this post is a study note translated to Chinese by me. Click [here](https://wei2624.github.io/MachineLearning/sv_discriminative_model/) to see the original English version in Wei's homepage.

请注意: 本文是我翻译的一份学习资料，英文原版请点击[Wei的学习笔记](https://wei2624.github.io/MachineLearning/sv_discriminative_model/)。

一类经典的学习问题是监督学习(supervised learning)，在这种情况下，我们有输入叫(features)，和输出(target)。学习的目的是基于给定的输入训练模型，然后让它预测输出。为此，我们收集一个训练数据集(training set)，其中我们有一对pair训练样本组成特征向量(feature vector)及其相应的输出(output)。 由于每一个输入都是基本事实，我们将这种学习称为监督学习(supervised learning)，将要训练的模型称为假设(hypothesis)。 下表是一个例子。
 
在这种情况下，我们将居住面积域作为特征和价格作为输出。学习任务是给定一个新房子居住面积的输入，该模型可以预测房子的价格。

当目标输出处于连续空间时，我们将其称为回归问题(regression problem)。 当目标输出位于离散空间时，我们将其称为分类问题(classification problem)。

###1线性回归 Linear Regression
以下是线性回归的模型
 
我们以θ0作为偏差项bias，有时称为截距项。 想象一下，当你尝试回归2D域中的线，截距项基本上决定了线与y轴交叉的位置。θ被称为我们想要从训练数据中学习的参数(parameters)。

为了训练模型，我们定义以下代价函数(cost function)，并试图最小化它：
 
目标是找到最小化代价的θ。 那么如何实现呢？为什么函数前面会有1/2呢？在下一节中我们推导出该成本函数的导数时将会很清楚。 简而言之，这种定义方式使数学运算变得方便。

最小均方差Least Mean Square(LMS) algorithm
LMS算法主要使用梯度下降gradient descent来找到局部最小值(local minimum)。 为了实现它，我们将参数初始化为 然后重复更新为：
 
其中j跨越特征向量中的所有个体。α被称为学习率(learning rate)，它控制模型学习/训练的速度。

现在，我们根据一个样本求偏导数：
 
数学解释：第二行是导数的的链式规则(chain rule)。 在第三行，我根据定义扩展 。 在最后一行，因为我们只关心θj，所以其他一切都是不变的。

所以，所有样本的更新是：
 
其中m是训练样本的数量，j可以跨越特征向量的维度。 该算法从每个训练样本中获取所有信息。 我们称之为批量梯度下降batch gradient descent。 该方法对局部最小值(即可能到达的鞍点saddle point）敏感，而我们通常假设成本函数仅具有全局最小值(global minimum)(J是凸函数convex function) 。 图形说明如下所示：
 
请注意，在更新过程中，我们会遍历所有样本以向局部最小值前进一步。 如果m非常大，则该步骤计算量十分巨大。 因此，在这种情况下，我们引入了一个类似算法，称为随机梯度下降stochastic gradient descent。其中只有一小部分样本一次被送入算法。 这样做使模型可以更快收敛converge，尽管它可能会在最小值处振荡(并没有严格到达最小)。 这种一步步更新来找到最小值叫迭代算法iterative algorithm。它将产生良好的对全局最小值的近似。 因此，我们经常在现实中使用它。

当不能直接计算或者很难计算使目标函数的导数为0的参数时，我们会用以上的迭代算法。如果可以直接计算使导数为0的参数，我们可以通过下面的正则方程直接计算。

#2 正则方程 Normal Equations
回想一下求函数最值的方法，我们可以设函数的导数为0来求得函数的最值，这样我们可以明确地计算局部最小值，而不是通过多次迭代。首先让我们来复习一下数学！~

矩阵导数
这里有一些相关概念概念

在本小节中，我将讨论线性代数中trace的计算，trace被定义为：
 
其中A必须是方阵。 现在，如果时间允许，我将列出跟踪和写入证明的属性。
 
证明：
 

 
证明：
 

另一个证明类似。 请注意，由于维度约束，你无法随机调整每个矩阵的顺序。

 
证明与上面类似

 
证明
 

 
证明与上面类似。

 
证明与上面类似。

 
证明：
 
我们知道：
 
带入可得
 

 
证明，假设 ，可得：
 

 
证明：trace只存在于方矩阵，，因此可得 
 
 
 

 

再看Least Square

因此，现在我们不是迭代地找到解决方案，而是明确地计算成本函数的导数，并将其设置为零以便一次性生成解。

我们定义训练集输入为：
 
输出为
 
定义模型为 ，我们得到
 
因此
 
所以在这一点上，我们需要找到关于θ的J的导数。 从trace的属性来看，我们知道：
 
我们知道scalar的trace是它自己，因此
 
解释： ，因此可得第二行。第三行（1） 对于θ求导可得0；（2） ；（3） 。第四行来源于 
我们将它设为0，可得正则方程，即：
 
因此我们可算出使矩阵导数为0的θ为
 

3概率解释
正则方程是找到解的一种确定性方法，让我们看看如何从概率的角度解释它。它最终应该得到相同的结果。

我们知道输入和输出的关系为：
 
其中 是随机变量，可以捕获噪声和非模型的因素。 这通常是线性回归的概率模型。 我们还假设噪声是i.i.d. 来自高斯分布，均值为0和一些方差 ，这是一种传统的建模方式。 结果是 是高斯的随机变量，并且 对于r.v.。 向高斯r.v.添加常数将使r.v.移动常数数量，但它仍然是高斯分布，只是具有不同的均值和相同的方差。因此，按照高斯分布的定义，我们可以说：
 
当x已知并具有固定参数θ时，该函数可被视为y的函数。 因此，我们可以称之为似然函数likelihood function：
 
我们需要找到满足以下条件的θ：选定θ的情况下，y基于给定x的概率最大化。 我们称之为最大可能性。 为简化，我们找到了最大对数似然log likelihood：
 
关于θ，最大化以上值与最小化J会得出相同的答案。这意味着我们用概率的方式证明我们在LMS中所得的结果。
 
4局部加权线性回归
在上面讨论的回归方法中，我们平等对待在训练样本过程中产生的代价。 但是，这可能不合适，因为一些异常值(outlier)应该减少权重。 因此我们根据每个样本的querying point来放置它的权重。 例如，这样的权重可以是：
 
虽然这类似于高斯，但它们无关。 x是querying point。 我们需要保留所有训练数据以进行新的预测。

5 分类与逻辑回归
我们可以将逻辑回归当做成一个特殊的回归问题，我们只回归到一组二进制值0和1。有时，我们也使用-1和1表示法，我们分别称它为负类和正类。

然而，如果我们在这里应用线性回归模型，那么我们预测0和1以外的任何值是没有意义的。因此，我们将假设函数修改为：
 
其中g称为逻辑函数或sigmoid函数，它的图像为：
 
输出范围从0到1。 这直观地解释了为什么我们将其称为回归，因为它在连续的空间中输出。 但是，该值表示属于某一类的概率。 所以基本上它是一个分类器。

让我们来看看当我们采用逻辑函数的导数会是什么：
 
有了这个先修知识，接下来的问题是我们应该如何找到θ。 我们知道最小二乘回归可以从最大似然算法中导出，我们从这里继续。

我们认为：
 

其中y应为1或0。 假设样本是iid，我们有可能性函数：
 
使用log函数，可得
 
然后，我们可以使用梯度下降来优化可能性。 在更新中，我们应该有 。 注意我们有加号而不是减号，因为我们发现最大值不是最小值。 接下来求导数：
 
从第一行到第二行，我们使用上面导出的logistic函数的导数。 这为我们提供了特征向量上每个维度的更新规则。 虽然在这种情况下我们有与LMS相同的算法，但在这种情况下的假设是不同的。 当我们谈论广义线性化模型时，使用相同的等式并不奇怪。

6 跑个题：感知器学习算法
我们再之后的学习理论中会继续讨论，简单来讲，我们把假设函数调整为：
 
之前的更新方程保持不变，这就是感知器学习算法。

7 牛顿最大化方法
所以想象一下，我们想要找到函数f的根。 牛顿方法允许我们以二次速度完成这项任务。 这个想法是随机初始化x0并找到f（x0）的切线，标记为f'（x0）。 我们使用f'（x0）的根作为新x。 我们还将新x和旧x之间的距离定义为Δ。 这方面的一个例子可以显示为：
 
所以我们得到：
 
从这个想法得出，我们可以让f（x）= L'（θ）。 通过这种方式，我们可以更快地找到目标函数的最大值。 为了找到min的方法类似。

如果θ是矢量值，我们需要在更新中使用Hessian。 关于Hessian的更多细节可以在另一篇文章中找到。 简而言之，为了更新，我们有：
 
虽然它也可以二次收敛，但是计算起来可比梯度下降麻烦得多。

8广义线性模型和指数族



























































































































